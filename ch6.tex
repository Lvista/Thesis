% !TEX root = ch6.tex
%% ---DO NOT MODIFY BEGIN---
\ifx\allfiles\undefined
\input{./config}
\begin{document}
% \makecover
\else
\fi
%% ---DO NOT MODIFY END---
%-------------------第6章------------------------

\section{基于深度学习模型的动态手势识别研究}

前述的方案适合进行升空时的实时位置控制，而缺少诸如起飞，降落，返航之类的命令控制。为扩大可控维度，并结合目前的可穿戴方案，探索能识别动态手势的深度学习模型。在这一章我们首先对基于CTC算法的模型在变长序列上的泛化性能，然后改进了一个已有研究的模型，并进行对比。

\subsection{问题描述}

考虑由简单个手势组合成的序列手势，将该序列手势的变长传感器信号序列经过编码和解码得到手势的具体组合。另外，试图找到一种预测模型，使得通过学习基本手势就可以用于预测其派生组合手势，那么模型就不用针对新手势重新训练。

具体来说，考虑变长时间序列预测问题，令输入序列的最长为$w_{max}$，最短为$w_{min}$，则可定义输入空间和输出空间为：
\begin{align}
\mathcal{X} &= \bigcup_{w=w_{min}}^{w_{max}} \mathbb{R}^{n \times w}\\
\mathcal{Y} &= \bigcup_{w=w_{min}}^{w_{max}} \bigcup_{j=1}^{w} \mathbb{R}^{j}
\end{align}
每时刻的输入为$n\times 1$的向量，输入序列长度为$w$，因此转换输入空间为$\mathbb{R}^{n \times w}$。$j$为输出序列长度，其中输出序列长度 $j$ 满足 $j \leq w$。

对于任意输入序列$\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_w] \in \mathcal{X}$，定义预测函数：
$$f: \mathcal{X} \rightarrow \mathcal{Y}$$
使得$\mathbf{y} = f(\mathbf{X}) \in \mathcal{Y}$，其中$\mathbf{y}$为输出手势的编码。

对于监督学习模型，在实际应用中我们只能观测到有限的训练样本。给定观测数据集$\mathcal{D} =\{(\mathbf{X}_i, \mathbf{y}_i)\}_{i=1}^N$，其中每个观测样本$(\mathbf{X}_i, \mathbf{y}_i)$是从理论空间$\mathcal{X} \times \mathcal{Y}$中采样得到的，即$\mathbf{X}_i \in \mathcal{X}$，$\mathbf{y}_i \in \mathcal{Y}$，且$\mathcal{D} \subset \mathcal{X} \times \mathcal{Y}$。基于这些有限的观测样本，学习预测函数$\hat{f}_\theta: \mathcal{X} \rightarrow \mathcal{Y}$，使得$\hat{\mathbf{y}}_i=\hat{f}_\theta(\mathbf{X}_i)\approx\mathbf{y}_i$，$\forall (\mathbf{X}_i, \mathbf{y}_i) \in \mathcal{D}$。


\subsection{数据收集和数据增强}\label{ssec:data_clct}

关于收集手势信息，目前的流行方案主要有单IMU以及IMU+sEMG，考虑到sEMG对手势信息的贡献度有限\cite{li2022research}，同时为了降低输入信息量，以便在边缘设备部署，本研究考虑使用单IMU获取手势序列信息。

6位参与者（男性，22-27岁，均为右利手）使用？？节所示的原型设备进行收集，采样率为50Hz。参与者以坐姿进行手势收集，设备有线连接到计算机，计算机端使用图\ref{fig:HGR_Recorder}所示的应用收集数据，该应用的UI窗口有一个进度条（“状态显示”），用于提示数据写入的开始和终止。进度条的开始由监督者手动控制，达到100\%时自动终止。

\textbf{主数据集sqn\_3}\hspace{5pt}主数据集用于训练模型，测试集和训练集均为长度为3的序列手势，以评估模型的基本性能，用sqn\_3表示。一个样本的收集时间为3.5s，这是一个经验值，经过多次测试得出。收集数据为IMU的3轴陀螺仪和3轴线性加速度仪，共6组数据。手势为四个基本手势的排列组合，基本手势指手的上，下，左，右摆动，排列组合长度取3，比如上下右，下左下。在相邻的两个基本手势不能相同的前提下，共有36个手势。每个手势执行5次，因此共有$5\times 36\times 6 = 1080$个样本，图\ref{fig:6.2-pre_pro_data}是一个样本经过预处理计算后的折线图。

\textbf{副数据集sqn\_2和sqn\_4}\hspace{5pt}副数据集用于测试模型对变长手势序列的泛化性能，用于评估对序列长度为2和4的手势的识别性能，分别用sqn\_2和sqn\_4表示。对长度为2的序列手势采集12个手势。由于序列的组合数量会随着序列长度指数上升，对长度为4的序列手势仅随机指定手势，采集10个手势。每个执行5次。共22个手势，$6\times 22\times 5=660$个样本。

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/6.1-HGR_Recorder.png}
    \caption{手势数据采集工具}
    \label{fig:HGR_Recorder}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/6.2-prepro_data.pdf}
    \caption{预处理后的数据样本}
    \label{fig:6.2-pre_pro_data}
\end{figure}

收集一次样本的过程如下：

\begin{enumerate}
    \item 参与者入座后，确保周围空间充足，手部以一个自然舒适的姿势保持静止
    \item 参与者注视图\ref{fig:HGR_Recorder}中的进度条，当进度条开始出现条时，开始执行手势。
    \item 完成一次手势后，再次回到静止，直到图\ref{fig:HGR_Recorder}中的进度条达到100\%
\end{enumerate}

针对小样本的问题，我们进行了数据增强。

考虑到手势数据的空间特性，我们进行空间旋转变换，将原数据绕x轴（沿手的前臂轴向前）旋转一定角度$\alpha$。定义旋转矩阵为：
$$
A_{\alpha} = 0
\begin{bmatrix}
  1 & 0 & 0\\
  0 & cos(\alpha) & -sin(\alpha)\\
  0& sin(\alpha) & cos(\alpha)
\end{bmatrix}
$$
旋转角度$\alpha$是一个遵循高斯分布的随机机数：
$$
\alpha \sim \mathcal{N}(\mu, \sigma^2)
$$
在这里，$\mu$取0，$3\sigma$取已有研究的经验值$15^\circ$\cite{d2020transformer}，即$\sigma=5^\circ$，
令原加速度/陀螺仪的三轴向量为$\bm{x}$，则旋转后的向量为：
$$
\bm{x}_r = \bm{x} \cdot {A_{\alpha}}^T
$$
对每个样本生成5个增强样本，合计$6 \times 1080 = 6480$个样本。

\subsection{预处理和特征提取}\label{ssec:feature_ext}

% 首先，我们计算了加速度/陀螺仪三轴数据的合能量$m$：
% $$
% m = x^2+y^2+z^2
% $$
% 我们将合能量$m$作为一个额外特征输入，因此，共有$6+2=8$个原始特征。将这8个原始特征经过S-G滤波（Savitzky-Golay滤波器）得到预处理后的特征。

对于LSTM和GRU，为了减少计算复杂度，进行手动特征提取。对6个原始特征，通过步长为0.2s（10个数据帧），窗口大小为0.5s（25个数据帧）的滑动窗口，分别提取了以下高级特征：
\begin{itemize}
    \item 均值 : $\mu = \frac{1}{N}\sum_{i=1}^{N}x_i$
    \item 能量 ：$E = \sum_{i=1}^{N}x_i^2$
    \item 平均绝对值 ：$MAV = \frac{1}{N}\sum_{i=1}^{N}|x_i|$
    \item 均方根: $RMS = \sqrt{\frac{1}{N}\sum_{i=1}^{N}x_i^2}$
    \item 标准差: $\sigma =\sqrt{\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2}$
    \item 平均绝对偏差: $MAD=\frac{1}{N}\sum_{i=1}^{N}|x_i-\mu|$
    \item 斜率符号变化: $SSC = \sum_{i=2}^{N-1}f[(x_i-x_{i-1}\times (x_i-x_{i+1}))]$
    其中：
$$f(x) = 
\begin{cases} 
1,  & x > 0 \\
0,  & \text{others}\\
\end{cases}
$$
\end{itemize}

共$7\times 6=42$个特征输入到模型。

\subsection{模型和训练方法}\label{ssec:model_and_train}

我们主要验证两类模型。一种是基于CTC算法的模型，一种是基于自回归算法的模型。我们先介绍基于CTC算法的识别模型框架，然后进一步介绍4种基于不同特征提取模型的识别模型。接下来介绍Kavarthapu等人\cite{kavarthapu2017hand}提出的基于SED自回归算法的模型（以下简称SED-AR模型），以及本研究提出的改进模型。

\subsubsection{CTC算法}

基于CTC的模型框架如图\ref{fig:6.3-encoder_decoder}所示。在这里，原始输入是一个时序长度为$w$，样本大小为$n\times 1$的时序序列。$w$大小由采集样本实际时序长度决定，$n$为原始特征维度，在这里取6。在实践中，剪裁输入空间为$\mathbb{R}^{n\times w}$。

与传统的编码-解码结构不同，CTC解码算法直接从特征序列进行解码，模型（Model）将源输入特征序列进行特征转换，得到$c\times k$的概率分布矩阵（probability distribution, p.d.），然后对该矩阵进行最大概率分类（Argmax Classification）得到编码序列，编码序列通过后处理（Post-processing）得到目标输出。在这里，用一个例子说明这个后处理过程。如表\ref{tab:output_eg}所示是前述的概率分布矩阵示例，每一行代表一个数据帧（或数据段），显示了其在$c=5$个手势分类上的概率，每行概率合为1。

\begin{table}[htbp]
  \centering
  \caption{概率分布矩阵示例}
  \label{tab:output_eg}
  \begin{tabular}{c|ccccc}
    \hline
     \textbf{\makecell{Index}} & \textbf{Class 1} & \textbf{Class 2} & \textbf{Class 3} & \textbf{Class 4} & \textbf{Class 0} \\ 
    \hline
    1 & .03 & .04 & .10 & .08 & .85 \\ 
    2 & .12 & .03 & .08 & .12 & .68 \\ 
    3 & .82 & .07 & .01 & .06 & .04 \\ 
    4 & .80 & .01 & .04 & .07 & .04 \\ 
    5 & .12 & .06 & .01 & .06 & .75 \\ 
    6 & .21 & .04 & .68 & .06 & .01 \\ 
    7 & .03 & .05 & .11 & .09 & .81 \\ 
    8 & .01 & .06 & .09 & .05 & .89 \\ 
    \hline
  \end{tabular}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/6.3-encoder_decoder.pdf}
    \caption{CTC解码模型框架}
    \label{fig:6.3-encoder_decoder}
\end{figure}

对该矩阵进行最大概率分类（Argmax Classification），即$argmax$函数，得到编码后的输出$(0, 0, 1, 1, 0, 3, 0, 0)$，再按CTC解码\cite{graves2006connectionist}，通过去0和去重，例如上述例子经过CTC解码后得到手势序列$(1, 3)$，它的含义是[上划，左划]。

\subsubsection{CTC特征转换模型}\label{sssec:ctc_model}

在这里我们尝试了4种用于特征转换的模型，在图\ref{fig:6.3-encoder_decoder}中框架中相当于是Model的位置。

\textbf{CNN}\hspace{5pt}模型结构如图\ref{fig:6.3-stru-cnn}所示。因为CNN能自动提取特征，原始特征直接作为CNN模型的输入。模型采用基于一维卷积神经网络的时序特征提取架构进行端到端序列识别。模型首先通过特征预处理层将输入映射至$D$维隐藏空间，并结合ReLU激活、Dropout正则化和批归一化进行特征标准化。核心的一维卷积模块包含$N$层卷积结构，每个卷积层后接批归一化、ReLU激活和Dropout防止过拟合。通过模型得到的特征接着经过层归一化稳定训练过程，最终通过由两个全连接层构成的分类器。分类器第一层映射到$D/2$维，后接ReLU激活和Dropout防止过拟合，第二层映射到$c=5$维的概率分布。$k$与卷积核和卷积步长有关。

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{fig/6.4-stru-cnn.pdf}
    \caption{CNN模型结构}
    \label{fig:6.3-stru-cnn}
\end{figure}

\textbf{LSTM}\hspace{5pt} 模型结构如图\ref{fig:6.5-stru-lstm}所示，原始特征经过\ref{ssec:feature_ext}节的手动特征提取，得到的$n\times 7=42$维输入。之后通过一个全连接层将输入特征映射到$D$维隐藏空间，然后使用$N$层单向LSTM来捕捉序列的时序特征和长短期依赖关系。输出经过层归一化来稳定训练过程。最后通过与CNN相同的两层全连接分类器将LSTM的输出映射到$c=5$个类别的概率分布，层间用ReLU激活和Dropout防止过拟合。
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{fig/6.5-stru-lstm.pdf}
    \caption{LSTM/GRU模型结构}
    \label{fig:6.5-stru-lstm}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig/6.6-stru-transformer.pdf}
    \caption{Transformer模型结构}
    \label{fig:6.6-stru-transformer}
\end{figure}

\textbf{GRU}\hspace{5pt} 原始特征经过\ref{ssec:feature_ext}节的手动特征提取，并使用类似LSTM的模型结构，仅将核心分类层替换为GRU。

\textbf{Transformer}\hspace{5pt} 原始序列特征直接作为Transformer模型的输入。参考Vaswani\cite{vaswani2017attention}等人的研究，构建如图\ref{fig:6.6-stru-transformer}所示的模型。模型将输入特征通过线性层投影到$D$维的Transformer隐藏空间，然后加入位置编码来保持序列位置信息。为了平衡梯度，稳定训练，在加入位置编码前进行元素点乘$D^2$。遵循BERT等模型的标准做法，使用attention mask处理填充位置\cite{devlin2019bert}核心部分使用$N$层Transformer编码器，前馈网络维度为$4N$。最后通过输出投影层将编码后的特征映射到$c=5$个类别的概率分布。

\textbf{训练方法}\hspace{5pt} 以上模型均采用CTC损失来处理序列对齐问题。训练配置为：批大小8，学习率为0.0001，包含10个epoch的预热阶段，最大训练100个epoch，使用AdamW优化器（beta值为0.9和0.98），L2正则化权重衰减为0.0001，梯度裁剪最大范数为1.0，以及余弦退火学习率调度和15个epoch的早停机制来提升模型性能和稳定性。

\subsubsection{先行研究模型及改进}\label{sssec:sed_ar_ctc}

作为对比，我们对SED-AR模型进行了复现和改进，改进模型用SED-CTC表示。

\textbf{SED-AR模型}\hspace{5pt} 根据Kavarthapu\cite{kavarthapu2017hand}的研究，SED-AR模型采用经典的编码器-解码器架构，如图\ref{fig:6.7-stru-sed}，主要包含编码器（Encoder），选择层（Selective Layer）和解码器（Decoder）三个核心组件。首先通过全连接层将原始IMU特征映射到$D$维隐藏空间，后接层归一化，ReLU激活函数，Dropout层稳定训练和防止过拟合。之后将其输入$N$个双向LSTM（Bi-LSTM）捕获手势序列的时序依赖关系。将最后一层双向LSTM的细胞状态 (Cell State, $c\_t$)	和隐藏状态 (Hidden State, $h\_t$)输入选择层，选择层是一个全连接层（$2D\to2D$）。得到的各$2D$维的细胞状态 (Cell State)和隐藏状态 (Hidden State)设定为解码器的初始状态。解码器包含一个单向LSTM，输入词汇大小为$c=6$（4类手势+<START>标记+<END>标记），维度为$E$的嵌入层（Embedding）。LSTM层输出到两个全连接层（$2D\to D\to c(6)$），中间同样使用归一化层，ReLU激活函数，Dropout层稳定训练和防止过拟合。

与CTC解码不同，自回归解码是一个递归过程。上述模型得到的p.d.经过$argmax$解码得到一个时间步的类别标记。上述的解码器（图\ref{fig:6.7-stru-sed}中的Decoder）会执行$j+2$次解码（$j$为目标输出序列长度，加上<START>和<END>标记）。在训练阶段，真实目标序列逐个输入嵌入层；在预测阶段，则将前述通过模型得到的一个时间步的类别标记输入嵌入层。
得到的预测序列与真实序列用交叉熵损失计算。

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/6.7-stru-sed.pdf}
    \caption{SED-AR/CTC模型结构}
    \label{fig:6.7-stru-sed}
\end{figure}

\textbf{SED-CTC模型}\hspace{5pt} 改进模型SED-CTC，主要是将自回归算法改为CTC算法。在实现上，结构与前者大体一致，不同的是，解码器中的嵌入层替换为编码器的输出，并且参考CTC算法\cite{graves2006connectionist}，由模型得到的p.d.和真实序列计算CTC损失。

以上两个模型均使用与\ref{sssec:ctc_model}节相同的训练和优化方法。

\subsection{实验}

\subsubsection{CTC模型对比实验}

首先对\ref{sssec:ctc_model}节的4种模型进行可行性测试，分析使用不同特征提取模型，使用不同参数下的CTC模型性能。

\textbf{模型参数设置}\hspace{5pt}基于\ref{ssec:model_and_train}节的模型，设定表\ref{tab:model_para_setting}所示模型参数。主要对比了隐藏层数（$N$）和模型层（$D$）数两个重要参数对模型的性能影响。

对于CNN的两层模型（$N=2$），第一层使用3×1卷积核将通道数扩展至2倍隐藏维度，第二层采用扩张卷积（dilation=2, kernel size=3）压缩回原始维度，有效感受野达到5个时间步长，能够在保持参数效率的同时捕获长距离时序依赖。对于CNN的单层模型（$N=1$），只保留前一层。

对于Transformer模型，为了保持每个注意力头的维度合理，当隐藏层数从64调整到32时，注意力头数从8调整到4。

\begin{table}
    \centering
    \caption{4次测试的模型参数设置}
    \begin{tabular}{cccc}
    \hline
         \textbf{Test name}& \textbf{hidden\_dim($D$)} & \textbf{num\_layers($N$)}\\
         \hline
         T\_2L\_64 & 64 & 2  \\
         T\_1L\_64 & 64 & 1  \\
         T\_2L\_32 & 32 & 2  \\
         T\_1L\_64 & 32 & 1  \\
         \hline
    \end{tabular}
    \label{tab:model_para_setting}
\end{table}

\textbf{准确度性能测试}\hspace{5pt}使用\ref{ssec:data_clct}节收集的sqn\_3数据集进行训练和测试。使用配备GPU的工作站进行训练。使用Python的PyTorch框架构建模型，训练和测试脚本，使用PyTorch的DataLoader实现批量加载，设置随机种子保证可复现性，并启用CUDA加速。将6个人的数据分为1人用于测试，5人用于训练和验证。根据经验，其中的5人中4人（80\%）用于训练，1人（20\%）用于验证。使用双层交叉验证，外层循环逐对6人数据进行6次迭代，每次选择不同人员作为测试集，内层循环进行5次迭代，在剩余5人中轮流选择1人作为验证集，其余4人作为训练集。

记录每次外层循环（6次迭代）的测试集准确率，最终计算平均测试准确率作为评估指标。准确率定义为在测试集中预测正确的样本数量与测试集总样本数量的比值。

\textbf{泛化性能测试}\hspace{5pt}在sqn\_3数据集上训练好的模型，在sqn\_2和sqn\_4数据集上测试泛化性能。在测试阶段使用的sqn\_3，sqn\_2和sqn\_4数据集均是来自训练集外的的

\subsubsection{先行研究对比测试}

对\ref{sssec:sed_ar_ctc}节提到的SED-AR模型和SED-CTC模型进行前述相同的准确度性能测试，和泛化性能测试。该实验仅对一个参数配置下的模型进行对比。

\subsection{结果和讨论}

\subsubsection{CTC模型对比实验}

表\ref{tab:ch6-2l64}，\ref{tab:ch6-2l32}，\ref{tab:ch6-1l64}，\ref{tab:ch6-1l32}展示了4次测试的模型性能表现。

就测试T\_2L\_64来看，对于3个序列长度的手势，平均性能最好的是Transformer模型，三个数据集上的识别准确表现均达到了96\%以上。但其模型参数量达到了104.3K，是参数量最低的CNN（52.7K）的2倍左右。

其次是LSTM，达到了96\%左右的准确率，而GRU虽略低了一点，参数量确减少到了LSTM的77\%。 

\begin{table}[htbp]
    \centering
        \caption{CTC模型平均测试准确率（T\_2L\_64）}
    \begin{tabular}{ccccc}
        \hline
        模型 & 参数量 & sqn\_2 & sqn\_3 &sqn\_4\\
        \hline
        CNN & 52.7K & $1.0$ & $.96$& $.94$\\
        LSTM & 71.0K  & $.96$& $.96$ & $.96$\\
        GRU & 54.4K  & $.94$ & $.96$ & $.94$\\
        Transformer & 104.3K & $.96$ & $.98$ & $.97$\\
        \hline
    \end{tabular}

    \label{tab:ch6-2l64}

    \centering
        \caption{CTC模型平均测试准确率（T\_2L\_32）}
    \begin{tabular}{ccccc}
        \hline
        模型 & 参数量 & sqn\_2 & sqn\_3 &sqn\_4\\
        \hline
        CNN & 13.5K & $1.0$ & $.96$& $.95$\\
        LSTM & 18.6K  & $.94$& $.95$ & $.91$\\
        GRU & 14.4K  & $.94$ & $.96$ & $.95$\\
        Transformer & 27.0K & $.88$ & $.96$ & $.96$\\
        \hline
    \end{tabular}

    \label{tab:ch6-2l32}

    \centering
        \caption{CTC模型平均测试准确率（T\_1L\_64）}
    \begin{tabular}{ccccc}
        \hline
        模型 & 参数量 & sqn\_2 & sqn\_3 &sqn\_4\\
        \hline
        CNN & 15.4K & $.99$ & $.98$& $.93$\\
        LSTM & 37.7K  & $.96$& $.95$ & $.92$\\
        GRU & 14.4K  & $.96$ & $.95$ & $.93$\\
        Transformer & 54.3K & $.92$ & $.97$ & $.98$\\
        \hline
    \end{tabular}
    \label{tab:ch6-1l64}

    \centering
        \caption{CTC模型平均测试准确率（T\_1L\_32）}
    \begin{tabular}{ccccc}
        \hline
        模型 & 参数量 & sqn\_2 & sqn\_3 &sqn\_4\\
        \hline
        CNN & 4.1K & $.98$ & $.95$& $.93$\\
        LSTM & 10.2K  & $.94$& $.96$ & $.94$\\
        GRU & 8.1K  & $.93$ & $.95$ & $.95$\\
        Transformer & 14.3K & $.89$ & $.93$ & $.94$\\
        \hline
    \end{tabular}
    \label{tab:ch6-1l32}
\end{table}

CNN的性能和参数量与GRU相当，但在不同序列长度的数据集上表现差异明显。在短序列的sqn\_2上达到了100\%的准确率，但在长序列的sqn\_4上准确率下滑到94\%，后面的3次测试也表现出同样的准确率下滑现象，说明CNN对局部特征表征能力优秀，而对于长序列的泛化性能差，长程时序特征捕捉能力不足。

对比T\_2L\_32和T\_2L\_64，对于CNN和GRU，性能基本无影响，而性能下降最明显是LSTM和Transformer。其中LSTM对于长序列的sqn\_4下降到了91\%，Transformer对于短序列sqn\_2下降到了88\%。结合4次测试来看，LSTM的性能下降可能是模型训练的过拟合问题导致的，而Transformer的性能下降则可能是模型本身对短序列的特征分析能力不足导致的。虽然提高模型复杂度可以一定程度弥补这个不足，但该模型依赖于时序输入的充足的信息。

对于T\_1L\_32，除了Transformer外，三个模型的性能都在95\%左右，但正如前述，CNN在长序列和短序列上的表现差异明显。

总体来看，对于短序列，CNN表现最好，就算降低了模型复杂度，其对于sqn\_2数据集的识别精度也保持在98\%以上。对于长序列，Transformer表现最好，在四个模型中识别准确率最高（除了T\_1L\_32），但整体性能受模型复杂度影响明显。LSTM和GRU性能表现相近，在模型复杂度降低的情况下，其整体识别准确率依旧能保持1\%左右的浮动。

\subsubsection{先行研究对比测试}

表\ref{tab:ch6-sed}展示了SED-AR和SED-CTC模型的识别准确率。

意外的是，对于训练集只包含sqn\_3的条件下，SED-AR模型无法对其它长度的手势进行识别。究其原因，推测是自回归算法本身由于只递归学习了长度为3的手势，这意味着模型学习到<END>总是在第4个位生成。所以对于sqn\_2，本来应该是<END>的第3位被生成为某一有效输出，对于sqn\_4，本来应该是有效输出的第4位被生成为<END>。本次实验的实际输出中也印证了这一推断。而对于训练集合内的数据，准确率来到了88\%，与原论文对应的测试结果一致。

相比之下，SED-CTC模型能够正确识别训练集外的长度的样本。同时，在sqn\_3上识别准确率也比SED-AR高8\%，参数量也比SED-AR低29.1\%。

另外，对比表\ref{tab:ch6-sed}和表\ref{tab:ch6-2l64}可以看到，在参数量接近翻倍的情况下，SED的使用并没有提高模型的识别性能，相反，性能相比表\ref{tab:ch6-2l64}中的LSTM下降了，特别是对sqn\_2和sqn\_4，准确率下降了约5\%。推测是选择层的出现提高了模型对于训练数据的拟合能力，而对于训练数据外的样本则出现欠拟合。

综上所述，自回归算法可在训练数据相同长度集合内的输入进行识别，但无法对集合外的数据进行识别。而CTC算法则可以正确识别。另外，SED技术能提高模型的拟合能力，但也容易导致模型过拟合，降低模型对变长样本的泛化能力。

\begin{table}
    \centering
        \caption{原SED模型和改进模型平均测试准确率}
    \begin{tabular}{ccccc}
        \hline
        模型 & 参数量 & sqn\_2 & sqn\_3 &sqn\_4\\
        \hline
        SED-AR & 193.7K & $\times$ & $.88$& $\times$\\
        SED-CTC & 137.3K  & $.91$& $.97$ & $.92$\\
        \hline
    \end{tabular}
    \label{tab:ch6-sed}
\end{table}

\subsection{小结}

本章针对无人机手势控制中缺少命令控制维度的问题，研究了基于深度学习的动态手势识别方案。采用序列手势组合方式，避免了语义差异问题，具有良好的通用性和泛化性能。
通过6名参与者收集手势数据，构建了包含sqn\_2、sqn\_3、sqn\_4的多长度序列数据集，并采用空间旋转进行数据增强。基于CTC算法，对比分析了CNN、LSTM、GRU和Transformer四种深度学习模型在不同参数配置下的性能表现，还对比了先前研究模型SED-AR和本文的改进模型SED-CTC的性能表现。
实验结果表明：
\begin{enumerate}
    \item Transformer模型整体性能最优，在三个数据集上识别准确率均达96\%以上，但参数量较大（104.3K）；
    \item CNN在短序列识别上表现突出（sqn\_2达100\%），但长序列泛化能力有限；
    \item GRU模型在参数效率和性能平衡方面具有显著优势，参数量仅为LSTM的77\%，识别准确率保持96\%左右；
    \item 随着模型复杂度降低，CNN和GRU性能稳定，而Transformer对模型复杂度更为敏感。
    \item 自回归算法可识别训练数据集合内长度的样本，但无法识别集合外长度的样本。而CTC算法能正确识别。
    \item SED技术能提高模型的拟合性能，但会降低模型的泛化能力。
\end{enumerate}

本章研究的结果为边缘设备部署的无人机手势控制提供了可行方案。但目前只在计算机上进行了测试，仍需要在边缘设备上进一步实际测试模型性能。

%% ---DO NOT MODIFY BEGIN---
\ifx\allfiles\undefined
\end{document}
\fi
%% ---DO NOT MODIFY END---